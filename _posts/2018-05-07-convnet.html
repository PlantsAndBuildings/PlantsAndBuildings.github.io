---
title: convnet
layout: post
categories: [misc]
published: true
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    processEscapes: true
  }
});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h3>0. A convnet, you say? In C++?! FROM SCRATCH??!!</h3>
<p style="text-align: justify;">About a week ago, I was at work with absolutely nothing to do - even the office pool table was (is still) broken. If you subscribe to the creationist agenda, you might refer to what happened next as a divine calling. Being a Darwinist myself, I attribute it to mind numbing boredom crippling all rational thought processes. See, over the course of last week, I implemented a machine learning model - one that is notoriously hard to debug when things do not work - in, what perhaps is, the most unforgiving programming language known to man.</p>
<p style="text-align: justify;"><strong>On 18<sup>th</sup> April 2018, I decided to implement a convolutional neural network</strong></p>
<p style="text-align: justify;"><strong>... from scratch</strong></p>
<p style="text-align: justify;"><strong>... in C++</strong></p>
<p style="text-align: justify;">Maybe I thought that it would be a nice way to kill time; maybe I severely (<strong>SEVERELY</strong>) underestimated the magnitude of complexity involved; or maybe .. just maybe, I thought that it might lead to some sort of cosmic enlightenment about the nature of deep learning models (mysterious creatures that they are). As misguided as the foundations of my enterprise may have been, last night my efforts finally came to fruition when <strong>the</strong> <strong>Le Net I implemented scored 0.973 on the <a href="https://www.kaggle.com/c/digit-recognizer">Kaggle Digit Recognizer Challenge</a> leaderboard</strong>. What follows is an account of tears shed in the process.</p>
<p style="text-align: justify;"><strong>The end result can be found <a href="https://github.com/PlantsAndBuildings/cpp-cnn">on my Github</a></strong>. I do plan to add a useful README and instructions for building and running the network soon. In this post, I plan to focus on the aspects I found interesting and challenging about the process - the code on Github contains all the remaining missing parts.</p>
<h3 style="text-align: justify;">1. Which ones do I use?</h3>
<p style="text-align: justify">Right off the bat, I made some decisions about the tools I would use for the implementation:</p>
<p style="text-align: justify">1. I'd already decided that the <strong>implementation would be in C++</strong></p>
<p style="text-align: justify">2. I knew that I needed linear algebra utilities - and I knew of two libraries which could provide that for me: <strong><a href="http://arma.sourceforge.net/">Armadillo</a></strong> and <strong><a href="http://eigen.tuxfamily.org">Eigen</a></strong>. I'd had some experience with both, more with Eigen than Armadillo, but I would have had to install Eigen on my system - and I couldn't be bothered to do that. <strong>Hence, Armadillo.</strong></p>
<p style="text-align: justify">3. I decided (and I still pat myself on the back for this decision) to properly test the code as I was writing it. I decided to use the <strong><a href="https://www.boost.org/doc/libs/1_53_0/libs/test/doc/html/utf.html">Boost Test Framework</a></strong> to write unit tests.</p>
<p style="text-align: justify">A few days into the project I also decided to use the <strong><a href="https://cmake.org/">CMake</a></strong> build system to easily build and run my code.</p>
<h3 style="text-align: justify;">2. Forward and Backward</h3>
<p style="text-align: justify;">I started right in the middle of everything - the convolution layer was the first thing I wanted to do. After a quick constructor implementation, I came to the Forward and Backward passes. The Forward pass was pretty straight-"forward"... See what I did there? <strong>There are several excellent explanations <a href="https://www.youtube.com/watch?v=bNb2fEVKeEo&amp;t=0s&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=5">(one can be found here)</a> of the forward pass, and TBH it's not that tough to understand, so I won't go into it here.</strong> I'll just paste my code:</p>

<pre class="language-cpp"><code>void Forward(arma::cube&amp; input, arma::cube&amp; output)
{
  // The filter dimensions and strides must satisfy some contraints for
  // the convolution operation to be well defined.
  assert((inputHeight - filterHeight)%verticalStride == 0);
  assert((inputWidth - filterWidth)%horizontalStride == 0);

  // Output initialization.
  output = arma::zeros((inputHeight - filterHeight)/verticalStride + 1,
                       (inputWidth - filterWidth)/horizontalStride + 1,
                       numFilters);

  // Perform convolution for each filter.
  for (size_t fidx = 0; fidx &lt; numFilters; fidx++)
  {
    for (size_t i=0; i &lt;= inputHeight - filterHeight; i += verticalStride)
      for (size_t j=0; j &lt;= inputWidth - filterWidth; j += horizontalStride)
        output((i/verticalStride), (j/horizontalStride), fidx) = arma::dot(
            arma::vectorise(
                input.subcube(i, j, 0,
                              i+filterHeight-1, j+filterWidth-1, inputDepth-1)
              ),
            arma::vectorise(filters[fidx]));
  }
  
  // Store the input and output. This will be needed by the backward pass.
  this-&gt;input = input;
  this-&gt;output = output;
}</code></pre>


<p style="text-align: justify;">The backward pass needed some effort. The reason for that requires a slight digression:</p>
<p style="text-align: justify;">See, the issue with most texts on back-propagation (and ultimately most resources on neural networks that I've come across - <strong>with the notable exception of Andrej Karpathy's <a href="http://cs231n.stanford.edu/">CS231n</a></strong>) is that they explain things like gradients flowing backwards only in the context of dense layers. When I first started reading about backprop, I had this notion that it was a specialized mathematical technique developed by neural network scientists (?) to be applied only in the context of training neural networks - and that too fully connected feedforward networks. It is not that tough to grasp backpropagation in this context because all operations are dot products and sigmoid/softmax activations - and we all can calculate derivatives of these functions using high-school calculus. Then you find out about convnets and now you have this scary new operation called a convolution which involves a filter and sliding across an input - and it is certainly not intuitive how a gradient can be calculated. By the time one gets to reading about RNNs and LSTMs, they've given up all hope on trying to understand how the backward pass works. This ignorance is enabled, nay, encouraged by prebuilt, packaged, served-on-a-silver-platter layers the kind of which are provided by Keras, Caffe and jack-in-the-box deep learning framework.</p>
<p style="text-align: justify;">The real trick to figuring out the backward pass through a convolution layer is that there is no trick. You just have to roll up your sleeves, clench your jaw, muster up all your grit and write it out on a piece of paper - atleast that's how I did it (perhaps to a less dramatic narration). <strong>Deriving backpropagation equations for any layer on your own, without looking up anything on the Internet is (atleast in my experience) a very fulfilling experience. It opened my eyes to how general the method is - that it can be applied in any context (not just neural nets) where gradients of highly composite functions are needed.</strong></p>
<p style="text-align: justify;">Coming back to the Backward pass through a conv layer. <strong>Here is how I derived it</strong>:</p>
<p>Consider first, the case of a linear input with linear filters. Suppose the input size is 8 which is being convolved with a filter of size 3.</p>
<p>$$ \begin{bmatrix}x_1\\x_2\\x_3\\x_4\\x_5\\x_6\\x_7\\x_8\end{bmatrix} * \begin{bmatrix}f_1\\f_2\\f_3\end{bmatrix} = \begin{bmatrix} f_1 x_1 + f_2 x_2 + f_3 x_3\\f_1 x_2 + f_2 x_3 + f_3 x_4\\ f_1 x_3 + f_2 x_4 + f_3 x_5\\ f_1 x_4+ f_2 x_5+ f_3 x_6\\ f_1 x_5+ f_2 x_6+ f_3 x_7\\ f_1 x_6+ f_2 x_7+ f_3 x_8 \end{bmatrix} = \begin{bmatrix}y_1\\y_2\\y_3\\y_4\\y_5\\y_6\end{bmatrix} $$</p>

<p>Now, we see that the loss $ L $ is a function of $ y_1, y_2, y_3, y_4, y_5 $ and $ y_6 $</p>

<p>$$ L = L(y_1, y_2, y_3, y_4, y_5, y_6) $$</p>
<p style="text-align: justify;">In the backpropagation step, we <strong>use the gradient of loss with respect to the layer outputs to compute the gradients of the loss with respect to the layer inputs</strong>. It should be clear why the gradient of loss with respect to the output is already available. Hence, when the backward pass reaches the convolution layer, we already have $ \nabla_y L $. Two minor notes:</p>

<ol>
<li style="text-align: justify;">Just a refresher, $ \nabla_y L = \begin{bmatrix} \frac{\partial L}{\partial y_1}\\ \frac{\partial L}{\partial y_2}\\ \frac{\partial L}{\partial y_3}\\ \frac{\partial L}{\partial y_4}\\ \frac{\partial L}{\partial y_5}\\ \frac{\partial L}{\partial y_6}\\ \end{bmatrix}$</li>
<li style="text-align: justify;">Also, if it is not clear why this is already available when the backward pass reaches the convolution layer - consider this hastily made flowchart of forward and backward passes:<br /><img src="images/backprop.png" alt="" /></li>
</ol>
<p style="text-align: justify;">So, to leverage the math notation and state things concisely: <strong>we are given</strong> $ \nabla_y L $ <strong>and we must compute</strong> $ \nabla_x L $<strong> and</strong> $ \nabla_f L $<strong>.</strong> That doesn't seem too hard, does it? We can almost directly write out:</p>
